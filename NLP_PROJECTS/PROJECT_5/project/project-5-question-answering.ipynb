{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center> Project-5: Question Answering","metadata":{}},{"cell_type":"markdown","source":"**Case Description**\n\nIn this notebook we fine-tune one of the 🤗 Transformers model (DeBERTa) to a Question Answering (QA) task, which is the task of extracting the answer to a question from a given context. This model selects a span of the input passage as the answer, does not generate new text!\n\n**Task**\n\nExtracting the answer to a question with QA transformer model.\n\n**Data**: It is a [SberQuAD](https://arxiv.org/pdf/1912.09723) (Sberbank Question Answering Dataset)\n\n**ML/DL task**: NlP task - Question Answering\n\n*Training on GPU*\n\n`Attention!!! We use only part of the dataset in order to save time to training and GPU resources and memory.`","metadata":{}},{"cell_type":"markdown","source":"# 0. Install and Import","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install transformers # the huggingface library containing the general-purpose architectures for NLP\n!pip install datasets # the huggingface library containing datasets and evaluation metrics for NLP\n!pip install evaluate\n!pip install -U ipywidgets\n!pip install optuna\n!pip install -U accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:53:06.195679Z","iopub.execute_input":"2025-01-15T08:53:06.196103Z","iopub.status.idle":"2025-01-15T08:54:06.793833Z","shell.execute_reply.started":"2025-01-15T08:53:06.196061Z","shell.execute_reply":"2025-01-15T08:54:06.792706Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport gc\nimport random\nimport numpy as np\nimport pandas as pd\nimport collections\n\nfrom tqdm.auto import tqdm\nfrom IPython.display import display, HTML\n\nimport evaluate\nimport optuna\n\n# pytorch libraries\nimport torch\n\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, pipeline\nfrom datasets import load_dataset, DatasetDict, ClassLabel, Sequence\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# cache_dir = os.makedirs(\"cache\",exist_ok=True)\n# os.environ['TRANSFORMERS_CACHE'] = \"cache\"\n# os.environ['HF_DATASETS_CACHE'] = \"cache\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:06.795634Z","iopub.execute_input":"2025-01-15T08:54:06.795918Z","iopub.status.idle":"2025-01-15T08:54:37.214756Z","shell.execute_reply.started":"2025-01-15T08:54:06.795891Z","shell.execute_reply":"2025-01-15T08:54:37.213908Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"%%capture\nos.system('python -m pip install datasets --upgrade')\nos.environ['TRANSFORMERS_CACHE'] = \"/opt/ml/checkpoints/\"\nos.environ['HF_DATASETS_CACHE'] = \"/opt/ml/checkpoints/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:37.215828Z","iopub.execute_input":"2025-01-15T08:54:37.216816Z","iopub.status.idle":"2025-01-15T08:54:46.617452Z","shell.execute_reply.started":"2025-01-15T08:54:37.216772Z","shell.execute_reply":"2025-01-15T08:54:46.616713Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\nCollecting datasets\n  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.24.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: datasets\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.21.0\n    Uninstalling datasets-2.21.0:\n      Successfully uninstalled datasets-2.21.0\nSuccessfully installed datasets-3.2.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Fixing RANDOM_SEED to make experiment repetable\nRANDOM_SEED = 42\n\n# Set random seeds\ndef set_seed(seed):\n    \"\"\"\n    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n    installed).\n\n    Args:\n        seed (:obj:`int`): The seed to set.\n    \"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n#     tf.random.set_seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    \n    \nset_seed(RANDOM_SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:46.619636Z","iopub.execute_input":"2025-01-15T08:54:46.619931Z","iopub.status.idle":"2025-01-15T08:54:46.638372Z","shell.execute_reply.started":"2025-01-15T08:54:46.619903Z","shell.execute_reply":"2025-01-15T08:54:46.637708Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Fixing package versions to make experiment repetable\n!pip freeze > requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:46.639357Z","iopub.execute_input":"2025-01-15T08:54:46.639618Z","iopub.status.idle":"2025-01-15T08:54:49.263372Z","shell.execute_reply.started":"2025-01-15T08:54:46.639593Z","shell.execute_reply":"2025-01-15T08:54:49.262184Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Depending on your model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors.\n# Set those two parameters, then the rest of the notebook should run smoothly:\nmodel_checkpoint = \"timpal0l/mdeberta-v3-base-squad2\" # Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing\nBATCH_SIZE = 10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:49.265123Z","iopub.execute_input":"2025-01-15T08:54:49.265897Z","iopub.status.idle":"2025-01-15T08:54:49.270331Z","shell.execute_reply.started":"2025-01-15T08:54:49.265831Z","shell.execute_reply":"2025-01-15T08:54:49.269480Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# If we have a GPU available, we'll set our device to GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:49.271762Z","iopub.execute_input":"2025-01-15T08:54:49.272131Z","iopub.status.idle":"2025-01-15T08:54:49.340103Z","shell.execute_reply.started":"2025-01-15T08:54:49.272092Z","shell.execute_reply":"2025-01-15T08:54:49.338938Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# 1. Data Loading: dataset exploration","metadata":{}},{"cell_type":"markdown","source":"We will use the 🤗 [Datasets](https://github.com/huggingface/datasets) library to download the data. This can be easily done with the functions *load_dataset*:","metadata":{}},{"cell_type":"code","source":"# Load the SBERQUAD dataset - https://huggingface.co/datasets/kuznetsoffandrey/sberquad\ndataset_full = load_dataset(\"sberquad\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:49.341263Z","iopub.execute_input":"2025-01-15T08:54:49.341551Z","iopub.status.idle":"2025-01-15T08:54:53.375660Z","shell.execute_reply.started":"2025-01-15T08:54:49.341517Z","shell.execute_reply":"2025-01-15T08:54:53.374717Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/5.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4512be915f44e1f872d3ac6fb810d99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ad7d5e2842943d7aefcfc7d0fce07d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/3.43M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd02dab9b8ef4e4daff78147c63a3701"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/4.93M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38e19337eeb94f51972bcf0c2a43a8c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/45328 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac39c5badc364b058c376d75fdbe6fdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/5036 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"067b4774455e4d9eba78fbf84134ac40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/23936 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f07d776d3d5442a593d5429d15873607"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"dataset_full","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:53.376651Z","iopub.execute_input":"2025-01-15T08:54:53.376936Z","iopub.status.idle":"2025-01-15T08:54:53.382743Z","shell.execute_reply.started":"2025-01-15T08:54:53.376909Z","shell.execute_reply":"2025-01-15T08:54:53.381748Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 45328\n    })\n    validation: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 5036\n    })\n    test: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 23936\n    })\n})"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"The `datasets` object itself is `DatasetDict`, which contains one key for the training, validation and test sets.\n\nThere are several important fields here:\n* **context**: background information from which the model needs to extract the answer\n* **question**: the question a model should answer\n* **answers**: the starting location of the answer token and the answer text","metadata":{}},{"cell_type":"code","source":"# To access an actual element, you need to select a split first, then give an index:\nprint(\"Context: \", dataset_full[\"train\"][1][\"context\"])\nprint(\"\\nQuestion: \", dataset_full[\"train\"][1][\"question\"])\nprint(\"\\nAnswer: \", dataset_full[\"train\"][1][\"answers\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:53.387171Z","iopub.execute_input":"2025-01-15T08:54:53.387620Z","iopub.status.idle":"2025-01-15T08:54:53.570379Z","shell.execute_reply.started":"2025-01-15T08:54:53.387594Z","shell.execute_reply":"2025-01-15T08:54:53.569486Z"}},"outputs":[{"name":"stdout","text":"Context:  В протерозойских отложениях органические остатки встречаются намного чаще, чем в архейских. Они представлены известковыми выделениями сине-зелёных водорослей, ходами червей, остатками кишечнополостных. Кроме известковых водорослей, к числу древнейших растительных остатков относятся скопления графито-углистого вещества, образовавшегося в результате разложения Corycium enigmaticum. В кремнистых сланцах железорудной формации Канады найдены нитевидные водоросли, грибные нити и формы, близкие современным кокколитофоридам. В железистых кварцитах Северной Америки и Сибири обнаружены железистые продукты жизнедеятельности бактерий.\n\nQuestion:  что найдено в кремнистых сланцах железорудной формации Канады?\n\nAnswer:  {'text': ['нитевидные водоросли, грибные нити'], 'answer_start': [438]}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"We can see the answers are indicated by their start position in the text (here at character 438) and their full text, which is a substring of the context as we mentioned above.","metadata":{}},{"cell_type":"markdown","source":"`To get a sense of what the data looks like, the following function will show some examples picked randomly from the dataset and decoded back to strings:`","metadata":{}},{"cell_type":"code","source":"def show_random_elements(dataset, num_examples=5):\n    assert num_examples <= len(\n        dataset\n    ), \"Can't pick more elements than there are in the dataset.\"\n    \n    picks = []\n    \n    for _ in range(num_examples):\n        pick = random.randint(0, len(dataset) - 1)\n        \n        while pick in picks:\n            pick = random.randint(0, len(dataset) - 1)\n            \n        picks.append(pick)\n\n    df = pd.DataFrame(dataset[picks])\n    \n    for column, typ in dataset.features.items():\n        \n        if isinstance(typ, ClassLabel):\n            df[column] = df[column].transform(lambda i: typ.names[i])\n            \n        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n            df[column] = df[column].transform(\n                lambda x: [typ.feature.names[i] for i in x]\n            )\n            \n    display(HTML(df.to_html()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:53.571484Z","iopub.execute_input":"2025-01-15T08:54:53.571885Z","iopub.status.idle":"2025-01-15T08:54:53.581465Z","shell.execute_reply.started":"2025-01-15T08:54:53.571826Z","shell.execute_reply":"2025-01-15T08:54:53.580806Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"show_random_elements(dataset_full[\"train\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:53.582326Z","iopub.execute_input":"2025-01-15T08:54:53.582571Z","iopub.status.idle":"2025-01-15T08:54:53.617226Z","shell.execute_reply.started":"2025-01-15T08:54:53.582548Z","shell.execute_reply":"2025-01-15T08:54:53.616322Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>context</th>\n      <th>question</th>\n      <th>answers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>36049</td>\n      <td>SberChallenge</td>\n      <td>В начале XX века Тебриз был пристанищем множества радикальных организаций, в силу чего играл важную роль в Конституционной революции каджарского Ирана 1905—1911 годов. Уже в 1908 году из города были изгнаны сторонники Мохаммед-Али шаха, который, будучи принцем, занимал пост губернатора Тебриза. Одним из главных требований протестующих было создание нового меджлиса. В ответ на революцию, английские и русские войска вторглись в Иран, чтобы её подавить. Последние заняли Тебриз, разоружили радикальные революционные группы, но при этом не признавали и отказывали возможности проехать в город шахскому губернатору. В итоге Мохаммед Али-шах вынужден был уехать в Россию, а к власти в 1909 году пришёл новый правитель Султан Ахмад-шах, последний из династии Каджаров[4].</td>\n      <td>Чьи сторонники были изгнаны в 1908 году во время протестов в Иране в начале XX века?</td>\n      <td>{'text': ['Мохаммед-Али шаха'], 'answer_start': [218]}</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>22571</td>\n      <td>SberChallenge</td>\n      <td>В России первый алмаз был найден 5 июля 1829 года на Урале в Пермской губернии на Крестовоздвиженском золотом прииске четырнадцатилетним крепостным Павлом Поповым, который нашёл алмаз, промывая золото в шлиховом лотке. За полукаратный кристалл Павел получил вольную. Павел привёл учёных, участников экспедиции немецкого учёного Александра Гумбольдта, на то место, где он нашёл первый алмаз (сейчас это место называется Алмазный ключик (по одноимённому источнику) и расположено приблизительно в 1 км от пос. Промысла́ недалеко от старой дороги, связывающей посёлки Промысла́ и Тёплая Гора Горнозаводского района Пермского края), и там было найдено ещё два небольших кристалла. За 28 лет дальнейших поисков был найден только 131 алмаз общим весом в 60 карат.</td>\n      <td>Что получил крепостной Павел Попов за находку алмаза?</td>\n      <td>{'text': ['Вольную.'], 'answer_start': [-1]}</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>68245</td>\n      <td>SberChallenge</td>\n      <td>В конце XVIII — начале XIX столетий в репертуаре театра появились оперы итальянских композиторов П. Анфосси, П. Гульельми, Д. Чимарозы, Л. Керудини, Дж. Паизиелло, С. Майра. В 1812 году на сцене театра состоялась премьера оперы Дж. Россини Пробный камень . Она положила начало так называемому россиниевскому периоду. Театр Ла Скала первым поставил его оперы Аурельяно в Пальмире (1813), Турок в Италии (1814), Сорока-воровка (1817) и др. Одновременно театр ставил широко известные оперы Россини.</td>\n      <td>В каком году состоялась премьера оперы Дж. Россини Пробный камень ?</td>\n      <td>{'text': ['1812'], 'answer_start': [176]}</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>71864</td>\n      <td>SberChallenge</td>\n      <td>Бассейн каждого водоёма включает в себя поверхностный и подземный водосборы. Поверхностный водосбор представляет собой участок земной поверхности, с которого поступают воды в данную речную систему или определённую реку. Подземный водосбор образуют толщи рыхлых отложений, из которых вода поступает в речную сеть. В общем случае поверхностный и подземный водосборы не совпадают. Но так как определение границы подземного водосбора практически очень сложно, то за величину речного бассейна принимается только поверхностный водосбор.</td>\n      <td>Что образует подземный водосбор?</td>\n      <td>{'text': ['толщи рыхлых отложений, из которых вода поступает в речную сеть'], 'answer_start': [375]}</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>35398</td>\n      <td>SberChallenge</td>\n      <td>Институционализации, как показывают П. Бергер и Т. Лукман, предшествует процесс хабитуализации, или опривычивания повседневных действий, приводящий к формированию образцов деятельности, которые в дальнейшем воспринимаются как естественные и нормальные для данного рода занятий или решения типичных в данных ситуациях проблем. Образцы действий выступают, в свою очередь, основой для формирования социальных институтов, которые описываются в виде объективных социальных фактов и воспринимаются наблюдателем как социальная реальность (или социальная структура). Эти тенденции сопровождаются процедурами сигнификации (процесс создания, употребления знаков и фиксации значений и смыслов в них) и формируют систему социальных значений, которые, складываясь в смысловые связи, фиксируются в естественном языке. Сигнификация служит целям легитимации (признание правомочным, общественно признанным, законным) социального порядка, то есть оправдания и обоснования привычных способов преодоления хаоса деструктивных сил, угрожающих подорвать стабильные идеализации повседневной жизни.</td>\n      <td>Как в дальнейшем воспринимаются образцы деятельности, сформированные в процессе хабитуализации?</td>\n      <td>{'text': ['Как естественные и нормальные для данного рода занятий.'], 'answer_start': [-1]}</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"code","source":"\"\"\"\nIf we use the whole dataset we'll lose a lot of time at the training stage - around 4 hours!!!\nIt will be better to use small part of the dataset in order to look how optuna hyperparams optimization works\n\"\"\"\nds_part = load_dataset(\"sberquad\", split='train[:8000]+validation[:4000]')\nds_train_val = ds_part.train_test_split(train_size=8000, seed=RANDOM_SEED)\n\nds_devtest = ds_train_val['test'].train_test_split(test_size=0.5, seed=RANDOM_SEED)\n\nds_final = DatasetDict({\n    'train': ds_train_val['train'],\n    'validation': ds_devtest['train'],\n    'test': ds_devtest['test']\n})\n\nprint(\"Part of the dataset for project: \\n\", ds_final)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:53.618208Z","iopub.execute_input":"2025-01-15T08:54:53.618457Z","iopub.status.idle":"2025-01-15T08:54:55.002452Z","shell.execute_reply.started":"2025-01-15T08:54:53.618431Z","shell.execute_reply":"2025-01-15T08:54:55.001653Z"}},"outputs":[{"name":"stdout","text":"Part of the dataset for project: \n DatasetDict({\n    train: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 8000\n    })\n    validation: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 2000\n    })\n    test: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 2000\n    })\n})\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# 2. Data preprocessing","metadata":{}},{"cell_type":"markdown","source":"## 2.1 Data preprocessing: tokenization\nBefore we can feed those texts to our model, we need to preprocess them. This is done by a 🤗 Transformers Tokenizer which will tokenize the inputs and put it in a format the model expects, as well as generate the other inputs that model requires.\n\nWe want to get data at the next format:\r\n\r\n[CLS] question [SEP] context [SEP]","metadata":{}},{"cell_type":"markdown","source":"To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n- we get a tokenizer that corresponds to the model architecture we want to use,\n- we download the vocabulary used when pretraining this specific checkpoint","metadata":{}},{"cell_type":"code","source":"\"\"\"\nThat vocabulary will be cached, so it's not downloaded again the next time we run the cell\n\"\"\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:55.003730Z","iopub.execute_input":"2025-01-15T08:54:55.004006Z","iopub.status.idle":"2025-01-15T08:54:57.017483Z","shell.execute_reply.started":"2025-01-15T08:54:55.003976Z","shell.execute_reply":"2025-01-15T08:54:57.016539Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/453 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b30a9c962621416eba1f244d59155f50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"437bbb6a95264ad2b2b1d967e94a0768"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c0df5577fb048a29338e3dc47e4128a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/173 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec2e877cceeb4f4d990737f5bccc0986"}},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"The following assertion ensures that our tokenizer is a fast tokenizer from the 🤗 Tokenizers library.\nThose fast tokenizers are available for almost all models, and we will need some of the special features they have for our preprocessing\n\n`Checking which type of models have a fast tokenizer available and which don't in the` [big table of models](https://huggingface.co/docs/transformers/index#bigtable)","metadata":{}},{"cell_type":"code","source":"assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:57.018726Z","iopub.execute_input":"2025-01-15T08:54:57.019111Z","iopub.status.idle":"2025-01-15T08:54:57.023773Z","shell.execute_reply.started":"2025-01-15T08:54:57.019069Z","shell.execute_reply":"2025-01-15T08:54:57.022920Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"\"\"\"\nCall this tokenizer on two sentences (one for the answer, one for the context)\n\"\"\"\ncontext = ds_final[\"train\"][1][\"context\"]\nquestion = ds_final[\"train\"][1][\"question\"]\n\ninputs = tokenizer(question, context)\nprint(inputs)\nprint(\"\\n\")\nprint(tokenizer.decode(inputs[\"input_ids\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:57.025032Z","iopub.execute_input":"2025-01-15T08:54:57.025290Z","iopub.status.idle":"2025-01-15T08:54:57.046319Z","shell.execute_reply.started":"2025-01-15T08:54:57.025264Z","shell.execute_reply":"2025-01-15T08:54:57.045468Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': [1, 260, 15089, 43544, 434, 51646, 260, 53046, 3588, 140065, 148362, 6111, 280, 31476, 11408, 260, 230129, 309, 57683, 106364, 309, 15204, 151261, 42291, 138732, 260, 292, 2, 13574, 17355, 1966, 21785, 834, 260, 230129, 309, 42291, 138732, 43544, 167176, 3753, 140065, 148362, 6111, 280, 260, 1803, 31476, 11408, 261, 5779, 426, 658, 32610, 19529, 3927, 9329, 355, 311, 40966, 325, 69370, 412, 62045, 260, 25073, 13821, 42257, 82714, 13821, 14721, 11457, 19452, 544, 389, 149103, 98313, 1610, 545, 55329, 260, 76030, 818, 261, 260, 32307, 316, 108608, 10187, 13676, 15352, 260, 45251, 66584, 6900, 1840, 125999, 4190, 80933, 260, 412, 260, 230129, 325, 42291, 15501, 325, 262, 1050, 316, 18159, 355, 27993, 280, 260, 1803, 82125, 188845, 31300, 3672, 260, 153246, 947, 7749, 51780, 265, 178852, 355, 918, 1499, 1012, 260, 180553, 36660, 12296, 1012, 14746, 260, 196587, 42291, 15501, 260, 262, 3744, 170095, 311, 84684, 102369, 8491, 833, 260, 1012, 426, 74412, 355, 29271, 325, 260, 188770, 1610, 260, 412, 5900, 262, 893, 42291, 15501, 1159, 98472, 61676, 921, 316, 138352, 1610, 10406, 280, 389, 545, 55329, 261, 636, 76002, 508, 262, 25094, 1294, 29271, 434, 316, 49928, 61088, 75631, 41565, 355, 80299, 80476, 268, 779, 14583, 355, 262, 260, 3267, 11404, 44181, 3744, 3054, 389, 79127, 9571, 166752, 88189, 426, 10290, 23974, 833, 75280, 6252, 7334, 958, 134031, 80776, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n[CLS] Что вызывало множество пересудов среди современников лорда Джорджа Гордона Байрона?[SEP] Интимная жизнь лорда Байрона вызывала много пересудов среди его современников. Он покинул родную страну на фоне кривотолков относительно непозволительно близких отношений с единокровной сестрой Августой. Когда в 1860 году появилась книга графини Гвиччиоли о лорде Байроне, то в защиту памяти его супруги выступила миссис Бичер-Стоу со своей Истинной историей жизни леди Байрон, основанной на переданном будто ей по секрету рассказе покойной о том, что Байрон якобы состоял в преступной связи с сестрой. Впрочем, подобные рассказы вполне отвечали духу эпохи: к примеру, они составляют основное содержание автобиографической повести Шатобриана Рене (1802).[SEP]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"Depending on the model, it will be different keys in the dictionary returned by the cell above. They don't matter much for what we're doing here.","metadata":{}},{"cell_type":"code","source":"print(\"cls_token: \", tokenizer.cls_token)\nprint(\"sep_token: \", tokenizer.sep_token)\nprint(\"eos_token: \", tokenizer.eos_token)\nprint(\"pad_token: \", tokenizer.pad_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:57.047449Z","iopub.execute_input":"2025-01-15T08:54:57.047713Z","iopub.status.idle":"2025-01-15T08:54:57.052078Z","shell.execute_reply.started":"2025-01-15T08:54:57.047687Z","shell.execute_reply":"2025-01-15T08:54:57.051186Z"}},"outputs":[{"name":"stdout","text":"cls_token:  [CLS]\nsep_token:  [SEP]\neos_token:  [SEP]\npad_token:  [PAD]\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"So we see that the model checkpoint we're using uses the [CLS] token to denote the start of the question, then a [SEP] token to mark between the question and the context, and then is ended with another [SEP] token. This is in accordance with how SQUAD is defined.","metadata":{}},{"cell_type":"markdown","source":"## 2.2 Data preprocessing: sequence length and stride params\n`There are a few preprocessing steps particular to question answering tasks:`how to deal with very long documents.\n\n**Every transformer model has a maximum sequence length that it can handle**.\n\nWe usually truncate them in other tasks, when they are longer than the model maximum sentence length, but here, removing part of the the context might result in losing the answer we are looking for.\n\nTo deal with this, we will allow one (long) example in our dataset to give several input features, each of length shorter than the maximum length of the model (or the one we set as a hyper-parameter). Also, just in case the answer lies at the point we split a long context, we allow some overlap between the features we generate controlled by the hyper-parameter `doc_stride`","metadata":{}},{"cell_type":"code","source":"MAX_LENGTH = 384  # The maximum length of a feature (question and context)\nDOC_STRIDE = 128  # The allowed overlap between two part of the context when splitting is performed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:57.053229Z","iopub.execute_input":"2025-01-15T08:54:57.053885Z","iopub.status.idle":"2025-01-15T08:54:57.061946Z","shell.execute_reply.started":"2025-01-15T08:54:57.053834Z","shell.execute_reply":"2025-01-15T08:54:57.061146Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"\"\"\"\nLet's find one long example in our dataset:\n\"\"\"\nfor i, example in enumerate(ds_final[\"train\"]):\n    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n        break\n        \nexample = ds_final[\"train\"][i]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:57.063019Z","iopub.execute_input":"2025-01-15T08:54:57.063753Z","iopub.status.idle":"2025-01-15T08:54:57.086713Z","shell.execute_reply.started":"2025-01-15T08:54:57.063714Z","shell.execute_reply":"2025-01-15T08:54:57.085926Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"\"\"\"\nWithout any truncation, we get the following length for the input IDs:\n\"\"\"\nlen(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:57.087908Z","iopub.execute_input":"2025-01-15T08:54:57.088171Z","iopub.status.idle":"2025-01-15T08:54:57.093972Z","shell.execute_reply.started":"2025-01-15T08:54:57.088146Z","shell.execute_reply":"2025-01-15T08:54:57.093103Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"407"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"\"\"\"\nNow, if we just truncate, we will lose information (and possibly the answer to our question):\n\"\"\"\nlen(\n    tokenizer(\n        example[\"question\"],\n        example[\"context\"],\n        max_length=MAX_LENGTH,\n        truncation=\"only_second\",\n    )[\"input_ids\"]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:57.095028Z","iopub.execute_input":"2025-01-15T08:54:57.095286Z","iopub.status.idle":"2025-01-15T08:54:57.104511Z","shell.execute_reply.started":"2025-01-15T08:54:57.095262Z","shell.execute_reply":"2025-01-15T08:54:57.103821Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"384"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"To sum up key points:\n- Some examples in a dataset may have a very long context that exceeds the maximum input length of the model. To deal with longer sequences, truncate only the context by setting *truncation=\"only_second\"*. **We never want to truncate the question**, only the context, else the `only_second` truncation picked\n- Our tokenizer can automatically return a list of features capped by a certain maximum length, with the overlap we talked about above, we just have to tell it to do so with `return_overflowing_tokens=True` and by passing the stride\n- Map the start and end positions of the answer to the original context by setting `return_offset_mapping=True`\n- Use the `sequence_ids` method to find which part of the offset corresponds to the **question** and which corresponds to the **context**\n\nIn the labeled dataset, `answer_start` gives us the correponding location of the answer within the context string. Note that it's relative to the start of the context string, not the question + context. The **answer text** gives us the actual plaintext answer, from which we can easily calculate the `answer_end` position as just `answer_start` **plus the length of the answer**.\n\nThis format is not sufficient to train from — we'll need labels for both start and end positions.","metadata":{}},{"cell_type":"code","source":"tokenized_example = tokenizer(\n    example[\"question\"],\n    example[\"context\"],\n    max_length=MAX_LENGTH,\n    truncation=\"only_second\",\n    return_overflowing_tokens=True,\n    stride=DOC_STRIDE,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:57.105368Z","iopub.execute_input":"2025-01-15T08:54:57.105606Z","iopub.status.idle":"2025-01-15T08:54:57.124538Z","shell.execute_reply.started":"2025-01-15T08:54:57.105582Z","shell.execute_reply":"2025-01-15T08:54:57.123901Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"\"\"\"\nNow we don't have one list of input_ids, but several:\n\"\"\"\n[len(x) for x in tokenized_example[\"input_ids\"]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:57.125638Z","iopub.execute_input":"2025-01-15T08:54:57.125992Z","iopub.status.idle":"2025-01-15T08:54:57.137111Z","shell.execute_reply.started":"2025-01-15T08:54:57.125954Z","shell.execute_reply":"2025-01-15T08:54:57.136338Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"[384, 171]"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"\"\"\"\nAnd if we decode them, we can see the overlap:\n\"\"\"\nfor x in tokenized_example[\"input_ids\"][:2]:\n    print(tokenizer.decode(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:57.138086Z","iopub.execute_input":"2025-01-15T08:54:57.138577Z","iopub.status.idle":"2025-01-15T08:54:57.152505Z","shell.execute_reply.started":"2025-01-15T08:54:57.138551Z","shell.execute_reply":"2025-01-15T08:54:57.151680Z"}},"outputs":[{"name":"stdout","text":"[CLS] В каком году Мопертюи писал о естественных модификациях?[SEP] Однако в то время были и натуралисты, которые размышляли об эволюционном изменении организмов, происходящем в течение длительного времени. Мопертюи писал в 1751 году о естественных модификациях, происходящих во время воспроизводства, накапливающихся в течение многих поколений и приводящих к формированию новых видов. Бюффон предположил, что виды могут дегенерировать и превращаться в другие организмы. Эразм Дарвин считал, что все теплокровные организмы возможно происходят от одного микроорганизма (или филамента ). Первая полноценная эволюционная концепция была предложена Жаном Батистом Ламарком в 1809 году в труде Философия зоологии. Ламарк считал, что простые организмы (инфузории и черви) постоянно самозарождаются. Затем эти формы изменяются и усложняют своё строение, приспосабливаясь к окружающей среде. Эти приспособления происходят за счёт прямого влияния окружающей среды путём упражнения или неупражнения органов и последующей передачи этих приобретённых признаков потомкам (позже эта теория получила название ламаркизм). Эти идеи были отвергнуты натуралистами, поскольку не имели экспериментальных доказательств. Кроме того, всё ещё были сильны позиции учёных, считавших, что виды неизменны, а их сходство свидетельствует о божеств[SEP]\n[CLS] В каком году Мопертюи писал о естественных модификациях?[SEP] Эти приспособления происходят за счёт прямого влияния окружающей среды путём упражнения или неупражнения органов и последующей передачи этих приобретённых признаков потомкам (позже эта теория получила название ламаркизм). Эти идеи были отвергнуты натуралистами, поскольку не имели экспериментальных доказательств. Кроме того, всё ещё были сильны позиции учёных, считавших, что виды неизменны, а их сходство свидетельствует о божественном замысле. Одним из самых известных среди них был Жорж Кювье.[SEP]\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"It's going to take some work to properly label the answers here: we need to find in which of those features the answer actually is, and where exactly in that feature.\n\nThe models we will use require the **start and end positions of these answers** in the tokens, so we will also need to map parts of the original context to some tokens. Thankfully, the tokenizer we're using can help us with that by returning an `offset_mapping`","metadata":{}},{"cell_type":"code","source":"tokenized_example = tokenizer(\n    example[\"question\"],\n    example[\"context\"],\n    max_length=MAX_LENGTH,\n    truncation=\"only_second\",\n    return_overflowing_tokens=True,\n    return_offsets_mapping=True,\n    stride=DOC_STRIDE,\n)\nprint(tokenized_example[\"offset_mapping\"][0][:100])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:57.153448Z","iopub.execute_input":"2025-01-15T08:54:57.153691Z","iopub.status.idle":"2025-01-15T08:54:57.165651Z","shell.execute_reply.started":"2025-01-15T08:54:57.153666Z","shell.execute_reply":"2025-01-15T08:54:57.164783Z"}},"outputs":[{"name":"stdout","text":"[(0, 0), (0, 1), (1, 5), (5, 7), (7, 12), (12, 15), (15, 18), (18, 20), (20, 21), (21, 27), (27, 28), (28, 29), (29, 39), (39, 42), (42, 46), (46, 54), (54, 55), (55, 56), (0, 0), (0, 1), (1, 6), (6, 8), (8, 11), (11, 17), (17, 21), (21, 22), (22, 23), (23, 24), (24, 32), (32, 36), (36, 37), (37, 38), (38, 45), (45, 49), (49, 52), (52, 56), (56, 59), (59, 61), (61, 67), (67, 69), (69, 72), (72, 73), (73, 81), (81, 82), (82, 91), (91, 93), (93, 94), (94, 100), (100, 105), (105, 107), (107, 109), (109, 112), (112, 117), (117, 120), (120, 129), (129, 137), (137, 138), (138, 141), (141, 144), (144, 146), (146, 147), (147, 153), (153, 155), (155, 156), (156, 160), (160, 165), (165, 166), (166, 167), (167, 177), (177, 180), (180, 184), (184, 192), (192, 193), (193, 194), (194, 200), (200, 205), (205, 207), (207, 210), (210, 216), (216, 220), (220, 228), (228, 232), (232, 233), (233, 236), (236, 238), (238, 242), (242, 249), (249, 251), (251, 254), (254, 259), (259, 264), (264, 266), (266, 272), (272, 276), (276, 277), (277, 278), (278, 279), (279, 285), (285, 289), (289, 291)]\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"This gives the corresponding start and end character in the original text for each token in our input IDs.\n\nThe very first token [CLS] has (0, 0) because it doesn't correspond to any part of the question/answer, then the second token is the same as the characters 0 to 1 of the question:","metadata":{}},{"cell_type":"code","source":"first_token_id = tokenized_example[\"input_ids\"][0][1]\noffsets = tokenized_example[\"offset_mapping\"][0][1]\n\nprint(\n    tokenizer.convert_ids_to_tokens([first_token_id])[0],\n    example[\"question\"][offsets[0] : offsets[1]],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:57.166681Z","iopub.execute_input":"2025-01-15T08:54:57.166972Z","iopub.status.idle":"2025-01-15T08:54:57.175605Z","shell.execute_reply.started":"2025-01-15T08:54:57.166946Z","shell.execute_reply":"2025-01-15T08:54:57.174929Z"}},"outputs":[{"name":"stdout","text":"▁В В\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"So we can use this mapping to find the position of the start and end tokens of our answer in a given feature. We just have to distinguish which parts of the offsets correspond to the question and which part correspond to the context, this is where the `sequence_ids` method of our `tokenized_example` can be useful:","metadata":{}},{"cell_type":"code","source":"sequence_ids = tokenized_example.sequence_ids()\n\nprint(sequence_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:57.176611Z","iopub.execute_input":"2025-01-15T08:54:57.176958Z","iopub.status.idle":"2025-01-15T08:54:57.190374Z","shell.execute_reply.started":"2025-01-15T08:54:57.176923Z","shell.execute_reply":"2025-01-15T08:54:57.189539Z"}},"outputs":[{"name":"stdout","text":"[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"It returns `None` for the special tokens, then 0 or 1 depending on whether the corresponding token comes from the first sentence past (the question) or the second (the context).\n\nNow let's put everything together in one function we will apply to our training set.","metadata":{}},{"cell_type":"markdown","source":"## 2.3 Data preprocessing: training set","metadata":{}},{"cell_type":"code","source":"# Training set\ndef preprocess_training_examples(examples):\n    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    questions = [q.strip() for q in examples[\"question\"]]\n    \n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=MAX_LENGTH,\n        truncation=\"only_second\",\n        stride=DOC_STRIDE,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # The offset mappings will give us a map from token to character position in the original context. This will\n    # help us compute the start_positions and end_positions.\n    offset_mapping = inputs.pop(\"offset_mapping\")\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n    \n    answers = examples[\"answers\"]\n    # Let's label those examples!\n    start_positions = []\n    end_positions = []\n\n    for i, offset in enumerate(offset_mapping):\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_idx = sample_map[i]\n        answer = answers[sample_idx]\n        # Start/end character index of the answer in the text.\n        start_char = answer[\"answer_start\"][0]\n        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = inputs.sequence_ids(i)\n\n        # Find the start and end of the context\n        idx = 0\n        while sequence_ids[idx] != 1:\n            idx += 1\n        context_start = idx\n        while sequence_ids[idx] == 1:\n            idx += 1\n        context_end = idx - 1\n\n        # If the answer is not fully inside the context, label is (0, 0)\n        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n            start_positions.append(0)\n            end_positions.append(0)\n        else:\n            # Otherwise it's the start and end token positions\n            idx = context_start\n            while idx <= context_end and offset[idx][0] <= start_char:\n                idx += 1\n            start_positions.append(idx - 1)\n\n            idx = context_end\n            while idx >= context_start and offset[idx][1] >= end_char:\n                idx -= 1\n            end_positions.append(idx + 1)\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    return inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:57.194000Z","iopub.execute_input":"2025-01-15T08:54:57.194694Z","iopub.status.idle":"2025-01-15T08:54:57.205932Z","shell.execute_reply.started":"2025-01-15T08:54:57.194668Z","shell.execute_reply":"2025-01-15T08:54:57.205058Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# # This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:\n# features = preprocess_training_examples(ds_final[\"train\"][:5])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-14T17:27:32.037Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the `map` method of the dataset object we created earlier. Since our preprocessing changes the number of samples, we need to **remove the old columns** when applying it:","metadata":{}},{"cell_type":"code","source":"train_ds = ds_final[\"train\"].map(\n    preprocess_training_examples,\n    batched=True,\n    remove_columns=ds_final[\"train\"].column_names,\n    keep_in_memory=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:54:57.206803Z","iopub.execute_input":"2025-01-15T08:54:57.207086Z","iopub.status.idle":"2025-01-15T08:55:06.633808Z","shell.execute_reply.started":"2025-01-15T08:54:57.207061Z","shell.execute_reply":"2025-01-15T08:55:06.633050Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b58c72c292c4fb68dbeee76d8f7be31"}},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"train_ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:55:06.635124Z","iopub.execute_input":"2025-01-15T08:55:06.635397Z","iopub.status.idle":"2025-01-15T08:55:06.640587Z","shell.execute_reply.started":"2025-01-15T08:55:06.635371Z","shell.execute_reply":"2025-01-15T08:55:06.639798Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n    num_rows: 8414\n})"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"len(ds_final[\"train\"]), len(train_ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:55:06.641896Z","iopub.execute_input":"2025-01-15T08:55:06.642248Z","iopub.status.idle":"2025-01-15T08:55:06.657672Z","shell.execute_reply.started":"2025-01-15T08:55:06.642209Z","shell.execute_reply":"2025-01-15T08:55:06.656908Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"(8000, 8414)"},"metadata":{}}],"execution_count":31},{"cell_type":"markdown","source":"As we can see, the preprocessing added roughly 414 features. Our training set is now ready to be used — let’s dig into the preprocessing of the validation set!","metadata":{}},{"cell_type":"markdown","source":"## 2.4 Data preprocessing: validation/testing sets\n\nPreprocessing the validation data will be slightly easier as we don’t need to generate labels. The real joy will be to interpret the predictions of the model into spans of the original context. For this, we will just need to store both the offset mappings and some way to match each created feature to the original example it comes from. Since there is an ID column in the original dataset, we’ll use that ID.","metadata":{}},{"cell_type":"code","source":"# For validation and testing sets\ndef preprocess_validation_examples(examples):\n    questions = [q.strip() for q in examples[\"question\"]]\n    \n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=MAX_LENGTH,\n        truncation=\"only_second\",\n        stride=DOC_STRIDE,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n    example_ids = []\n\n    for i in range(len(inputs[\"input_ids\"])):\n        sample_idx = sample_map[i]\n        example_ids.append(examples[\"id\"][sample_idx])\n\n        sequence_ids = inputs.sequence_ids(i)\n        offset = inputs[\"offset_mapping\"][i]\n        inputs[\"offset_mapping\"][i] = [\n            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n        ]\n\n    inputs[\"example_id\"] = example_ids\n    return inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:55:06.658667Z","iopub.execute_input":"2025-01-15T08:55:06.658972Z","iopub.status.idle":"2025-01-15T08:55:06.672544Z","shell.execute_reply.started":"2025-01-15T08:55:06.658945Z","shell.execute_reply":"2025-01-15T08:55:06.671847Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"validation_ds = ds_final[\"validation\"].map(\n    preprocess_validation_examples,\n    batched=True,\n    remove_columns=ds_final[\"validation\"].column_names,\n    keep_in_memory=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:55:06.673639Z","iopub.execute_input":"2025-01-15T08:55:06.673998Z","iopub.status.idle":"2025-01-15T08:55:09.346057Z","shell.execute_reply.started":"2025-01-15T08:55:06.673961Z","shell.execute_reply":"2025-01-15T08:55:09.345172Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"578fac18aecb4056bc050d57650787e0"}},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"validation_ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:55:09.347101Z","iopub.execute_input":"2025-01-15T08:55:09.347369Z","iopub.status.idle":"2025-01-15T08:55:09.352948Z","shell.execute_reply.started":"2025-01-15T08:55:09.347342Z","shell.execute_reply":"2025-01-15T08:55:09.351973Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n    num_rows: 2086\n})"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"len(ds_final[\"validation\"]), len(validation_ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:55:09.354563Z","iopub.execute_input":"2025-01-15T08:55:09.355247Z","iopub.status.idle":"2025-01-15T08:55:09.372678Z","shell.execute_reply.started":"2025-01-15T08:55:09.355216Z","shell.execute_reply":"2025-01-15T08:55:09.371513Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"(2000, 2086)"},"metadata":{}}],"execution_count":35},{"cell_type":"markdown","source":"In this case we’ve only added 86 samples, so it appears the contexts in the validation dataset are a bit shorter.\n\nNow that we have preprocessed all the data, we can get to the training.","metadata":{}},{"cell_type":"markdown","source":"# 3. Model fine-tuning\n\nKey steps\n* Define metric computation function\n* Training model with base hyperparameters\n* Getting the best hyperparameters by optuna (automatic hyperparameter optimizations)\n* Training model with the best hyperparameters","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Metric computation\nSince we padded all the samples to the maximum length we set, there is no data collator to define, so this metric computation is really the only thing we have to worry about. The difficult part will be to post-process the model predictions into spans of text in the original examples; once we have done that, the metric from the 🤗 Datasets library will do most of the work for us.","metadata":{}},{"cell_type":"markdown","source":"We will use the 🤗 [Datasets](https://github.com/huggingface/datasets) library to get the metric we need to use for evaluation (to compare our model to the benchmark).\nUse for that the functions *load_metric*","metadata":{}},{"cell_type":"code","source":"# Define metric to compute\nmetric = evaluate.load(\"squad\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:55:09.374271Z","iopub.execute_input":"2025-01-15T08:55:09.374662Z","iopub.status.idle":"2025-01-15T08:55:10.240551Z","shell.execute_reply.started":"2025-01-15T08:55:09.374623Z","shell.execute_reply":"2025-01-15T08:55:10.239676Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52cf859c504b447ead25c79b0df46d37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f73b7e2fd1e145709309d7a588713e6b"}},"metadata":{}}],"execution_count":36},{"cell_type":"markdown","source":"The post-processing step will be similar to what we did there, so here’s a quick reminder of the actions we took:\n- We masked the start and end logits corresponding to tokens outside of the context.\n- We then converted the start and end logits into probabilities using a softmax.\n- We attributed a score to each (`start_index`, `end_index`) pair by taking the product of the corresponding two probabilities.\n- We looked for the pair with the maximum score that yielded a valid answer (e.g., a `start_index` lower than `end_index`).","metadata":{}},{"cell_type":"code","source":"def compute_metrics(start_logits, end_logits, features, examples):\n    # We need to find the predicted answer for each example in val_set.\n    # One example may have been split into several features in eval_set,\n    # so the first step is to map each example in val_set to the corresponding features in eval_set\n    example_to_features = collections.defaultdict(list)\n    for (idx, feature) in enumerate(features):\n        example_to_features[feature[\"example_id\"]].append(idx)\n\n    n_best = 20\n    max_answer_length = 30\n    predicted_answers = []\n    # we’ll look at the logit scores for the n_best start logits and end logits, excluding positions that give:\n    # - An answer that wouldn’t be inside the context\n    # - An answer with negative length\n    # \n    for example in tqdm(examples):\n        example_id = example[\"id\"]\n        context = example[\"context\"]\n        answers = []\n\n        # Loop through all features associated with that example\n        for feature_index in example_to_features[example_id]:\n            start_logit = start_logits[feature_index]\n            end_logit = end_logits[feature_index]\n            offsets = features[feature_index][\"offset_mapping\"]\n\n            start_indexes = np.argsort(start_logit)[-1:-n_best - 1:-1].tolist()\n            end_indexes = np.argsort(end_logit)[-1:-n_best - 1:-1].tolist()\n            \n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Skip answers that are not fully in the context\n                    if offsets[start_index] is None or offsets[end_index] is None:\n                        continue\n                    # Skip answers with a length that is either < 0 or > max_answer_length\n                    if (\n                        end_index < start_index\n                        or end_index - start_index + 1 > max_answer_length\n                    ):\n                        continue\n\n                    answer = {\n                        \"text\": context[offsets[start_index][0]:offsets[end_index][1]],\n                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n                    }\n                    answers.append(answer)\n\n        # Select the answer with the best score\n        if len(answers) > 0:\n            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n            predicted_answers.append(\n                {\"id\": str(example_id), \"prediction_text\": best_answer[\"text\"]}\n            )\n        else:\n            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n\n    # This metric expects the predicted answers in the format:\n    # a list of dictionaries with one key for the ID of the example and one key for the predicted text\n    # and the theoretical answers in the format: \n    # a list of dictionaries with one key for the ID of the example and one key for the possible answers:\n    theoretical_answers = [{\"id\": str(ex[\"id\"]), \"answers\": ex[\"answers\"]} for ex in examples]\n    \n    return metric.compute(predictions=predicted_answers,\n                          references=theoretical_answers)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:55:10.241903Z","iopub.execute_input":"2025-01-15T08:55:10.242178Z","iopub.status.idle":"2025-01-15T08:55:10.252166Z","shell.execute_reply.started":"2025-01-15T08:55:10.242152Z","shell.execute_reply":"2025-01-15T08:55:10.251294Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"Here we will need a bit more, as we have to look in the dataset of features for the offset and in the dataset of examples for the original contexts, so we won’t be able to use this function to get regular evaluation results during training. **We will only use it at the end of training to check the results**","metadata":{}},{"cell_type":"markdown","source":"## 3.2 Fine-tuning the model: base hyperparameters","metadata":{}},{"cell_type":"markdown","source":"Now that our data is ready for training, we can download the pretrained model and fine-tune it. Since our task is question answering, we use the `AutoModelForQuestionAnswering` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us:","metadata":{}},{"cell_type":"code","source":"model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:55:10.253445Z","iopub.execute_input":"2025-01-15T08:55:10.253741Z","iopub.status.idle":"2025-01-15T08:55:15.463835Z","shell.execute_reply.started":"2025-01-15T08:55:10.253714Z","shell.execute_reply":"2025-01-15T08:55:15.463140Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/879 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"934b8b11b64a482092a0318a2e2010bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d85ba6797d14eb48e0fff18fe0db98a"}},"metadata":{}}],"execution_count":38},{"cell_type":"markdown","source":"The warning is telling us that we should fine-tune this model before using it for inference, which is exactly what we are going to do.\n\nWe also tweak the learning rate, use the batch_size defined at the top of the notebook and customize the number of epochs for training, as well as the weight decay.","metadata":{}},{"cell_type":"code","source":"# TRAINING HYPERPARAMS\nLR = 2e-5\nNUM_EPOCHS = 5\nWD = 0.01","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:55:15.464914Z","iopub.execute_input":"2025-01-15T08:55:15.465182Z","iopub.status.idle":"2025-01-15T08:55:15.469180Z","shell.execute_reply.started":"2025-01-15T08:55:15.465157Z","shell.execute_reply":"2025-01-15T08:55:15.468316Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"training_args_base = TrainingArguments(\"bert-squad-base-params\",\n                                  evaluation_strategy=\"no\",\n                                  save_strategy=\"epoch\",\n                                  optim=\"adamw_torch\",\n                                  learning_rate=LR,\n                                  weight_decay=WD,\n                                  num_train_epochs=NUM_EPOCHS,\n                                  fp16=True,\n                                  seed=RANDOM_SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:55:15.470323Z","iopub.execute_input":"2025-01-15T08:55:15.470660Z","iopub.status.idle":"2025-01-15T08:55:15.548719Z","shell.execute_reply.started":"2025-01-15T08:55:15.470614Z","shell.execute_reply":"2025-01-15T08:55:15.547901Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"# Finally, we just pass everything to the Trainer class and launch the training\ntrainer_base = Trainer(model,\n                  training_args_base,\n                  train_dataset=train_ds,\n                  eval_dataset=validation_ds,\n                  tokenizer=tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:55:15.549799Z","iopub.execute_input":"2025-01-15T08:55:15.550083Z","iopub.status.idle":"2025-01-15T08:55:16.433575Z","shell.execute_reply.started":"2025-01-15T08:55:15.550056Z","shell.execute_reply":"2025-01-15T08:55:16.432654Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"torch.cuda.empty_cache()\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ['TRANSFORMERS_CACHE'] = \"/opt/ml/checkpoints/\"\nos.environ['HF_DATASETS_CACHE'] = \"/opt/ml/checkpoints/\"\n\n# 15/01/2025\ntrainer_base.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:55:22.586503Z","iopub.execute_input":"2025-01-15T08:55:22.587477Z","iopub.status.idle":"2025-01-15T09:52:13.673130Z","shell.execute_reply.started":"2025-01-15T08:55:22.587414Z","shell.execute_reply":"2025-01-15T09:52:13.672233Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2630' max='2630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2630/2630 56:46, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.714900</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.386800</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.176300</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.032000</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.937800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2630, training_loss=1.2314208404192906, metrics={'train_runtime': 3409.99, 'train_samples_per_second': 12.337, 'train_steps_per_second': 0.771, 'total_flos': 8244714800286720.0, 'train_loss': 1.2314208404192906, 'epoch': 5.0})"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"\"\"\"\nSave the model\n\"\"\"\ntrainer_base.save_model('./bert-squad-base-params-QA_15_01_25')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T09:52:20.447288Z","iopub.execute_input":"2025-01-15T09:52:20.447628Z","iopub.status.idle":"2025-01-15T09:52:23.539967Z","shell.execute_reply.started":"2025-01-15T09:52:20.447598Z","shell.execute_reply":"2025-01-15T09:52:23.538930Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"## 3.3 Evaluation","metadata":{}},{"cell_type":"code","source":"# Tokenize test set\ntest_ds = ds_final[\"test\"].map(\n    preprocess_validation_examples,\n    batched=True,\n    remove_columns=ds_final[\"test\"].column_names,\n)\n\nlen(ds_final[\"test\"]), len(test_ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T09:52:31.401776Z","iopub.execute_input":"2025-01-15T09:52:31.402175Z","iopub.status.idle":"2025-01-15T09:52:36.263610Z","shell.execute_reply.started":"2025-01-15T09:52:31.402143Z","shell.execute_reply":"2025-01-15T09:52:36.262685Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0417155bd1124f04a50c21eba1b2ed28"}},"metadata":{}},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"(2000, 2084)"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"# Getting prediction based on the test set\npredictions, _, _ = trainer_base.predict(test_ds)\nstart_logits, end_logits = predictions\n\ncompute_metrics(start_logits, end_logits, test_ds, ds_final[\"test\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T09:52:39.342973Z","iopub.execute_input":"2025-01-15T09:52:39.343328Z","iopub.status.idle":"2025-01-15T09:53:45.384799Z","shell.execute_reply.started":"2025-01-15T09:52:39.343298Z","shell.execute_reply":"2025-01-15T09:53:45.383904Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d2dbe758d6e4fc180081335e61cd5cc"}},"metadata":{}},{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"{'exact_match': 62.05, 'f1': 81.24410868559568}"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"# del trainer_base\n# gc.collect()\n# torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:51:37.939727Z","iopub.execute_input":"2025-01-14T15:51:37.940189Z","iopub.status.idle":"2025-01-14T15:51:38.559146Z","shell.execute_reply.started":"2025-01-14T15:51:37.940149Z","shell.execute_reply":"2025-01-14T15:51:38.558399Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"markdown","source":"## 3.4 Getting the best hyperparameters by Optuna","metadata":{}},{"cell_type":"code","source":"# DATASETS_for_optuna - Reduce the dataset to speed up the process of selecting hyperparameters\npart_of_data = 0.1\n\nDATASETS_for_optuna = DatasetDict({\n    'train': ds_final[\"train\"].map(\n        preprocess_training_examples,\n        batched=True,\n        keep_in_memory=True,\n        remove_columns=ds_final[\"train\"].column_names).select(\n            np.random.choice(range(len(ds_final[\"train\"])), int(len(ds_final[\"train\"])*part_of_data), replace=False)\n        ),\n    'validation': ds_final[\"validation\"].map(\n        preprocess_validation_examples,\n        batched=True,\n        keep_in_memory=True,\n        remove_columns=ds_final[\"validation\"].column_names).select(\n            np.random.choice(range(len(ds_final[\"validation\"])), int(len(ds_final[\"validation\"])*part_of_data), replace=False)\n    ),\n    'test': ds_final[\"test\"].map(\n        preprocess_validation_examples,\n        batched=True,\n        keep_in_memory=True,\n        remove_columns=ds_final[\"test\"].column_names).select(\n            np.random.choice(range(len(ds_final[\"test\"])), int(len(ds_final[\"test\"])*part_of_data), replace=False)\n    )\n})\n\nDATASETS_for_optuna","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T07:50:56.905320Z","iopub.execute_input":"2025-01-15T07:50:56.906181Z","iopub.status.idle":"2025-01-15T07:51:11.510941Z","shell.execute_reply.started":"2025-01-15T07:50:56.906137Z","shell.execute_reply":"2025-01-15T07:51:11.510202Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a45a2a40c8b44ebabc88eaa81196461"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3caa296625a24f46ade154c44266e719"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c10d3465ac64be7863dc4b498733339"}},"metadata":{}},{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n        num_rows: 800\n    })\n    validation: Dataset({\n        features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n        num_rows: 200\n    })\n    test: Dataset({\n        features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n        num_rows: 200\n    })\n})"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"# hyperparameters - https://python-bloggers.com/2022/08/hyperparameter-tuning-a-transformer-with-optuna/\nLR_MIN = 2e-5 # Learning rate minimum and maximum (ceiling) named LR_MIN and LR_CEIL\nLR_CEIL = 0.01\nWD_MIN = 4e-5 # Weight decay minimum and ceilling named WD_MIN and WD_CEIL\nWD_CEIL = 0.01\nWR_MIN = 0.01\nWR_CEIL = 0.2\nMIN_GRAD_ACC = 1\nMAX_GRAD_ACC = 5\nMIN_EPOCHS = 2 # Minimum and maximum epochs named MIN_EPOCHS and MAX_EPOCHS\nMAX_EPOCHS = 5\nPER_DEVICE_EVAL_BATCH = BATCH_SIZE # per device evaluation batch sizes for the training and evaluation sets\nPER_DEVICE_TRAIN_BATCH = BATCH_SIZE\nNUM_TRIALS = 3 # number of Optuna trials to implement – incrementing this will perform multiple hyperparameter trials for each individual permutation and setting\nSAVE_DIR = 'optuna-test' # SAVE_DIR is the name of the folder to save it to\nNAME_OF_MODEL = 'optuna_bp' # NAME_OF_MODEL is what I want to call my serialised and fine tuned transformer network","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T07:51:11.512286Z","iopub.execute_input":"2025-01-15T07:51:11.512549Z","iopub.status.idle":"2025-01-15T07:51:11.517536Z","shell.execute_reply.started":"2025-01-15T07:51:11.512524Z","shell.execute_reply":"2025-01-15T07:51:11.516502Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"def objective(trial: optuna.Trial):     \n    model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n    \n    training_args = TrainingArguments(         \n        output_dir=SAVE_DIR, \n        optim=\"adamw_torch\",\n        learning_rate=trial.suggest_loguniform('learning_rate', low=LR_MIN, high=LR_CEIL),         \n        weight_decay=trial.suggest_loguniform('weight_decay', WD_MIN, WD_CEIL),\n        warmup_ratio=trial.suggest_loguniform('warmup_ratio', WR_MIN, WR_CEIL),\n        gradient_accumulation_steps=trial.suggest_int('gradient_accumulation_steps', low = MIN_GRAD_ACC,high = MAX_GRAD_ACC),\n        num_train_epochs=trial.suggest_int('num_train_epochs', low = MIN_EPOCHS,high = MAX_EPOCHS),         \n        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH,         \n        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n        fp16 = True,  # for saving memory\n        # gradient_checkpointing=True, # for saving memory\n        seed = RANDOM_SEED,\n        lr_scheduler_type='cosine')\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=DATASETS_for_optuna['train'],\n        eval_dataset=DATASETS_for_optuna['validation'])      \n    \n    result = trainer.train()\n\n    del model, trainer\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return result.training_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T07:51:11.518590Z","iopub.execute_input":"2025-01-15T07:51:11.518942Z","iopub.status.idle":"2025-01-15T07:51:11.529885Z","shell.execute_reply.started":"2025-01-15T07:51:11.518905Z","shell.execute_reply":"2025-01-15T07:51:11.529093Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"def print_custom(text):\n    print('\\n')\n    print(text)\n    print('-'*100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T07:51:11.531467Z","iopub.execute_input":"2025-01-15T07:51:11.531737Z","iopub.status.idle":"2025-01-15T07:51:11.541320Z","shell.execute_reply.started":"2025-01-15T07:51:11.531714Z","shell.execute_reply":"2025-01-15T07:51:11.540528Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"print_custom('Triggering Optuna study')\nstudy = optuna.create_study(study_name='hp-search', direction='minimize') \nstudy.optimize(func=objective, n_trials=NUM_TRIALS)\n\n# This can be used to train the final model. Passed through using kwargs into the model\nprint_custom('Finding study best parameters')\nbest_lr = float(study.best_params['learning_rate'])\nbest_weight_decay = float(study.best_params['weight_decay'])\nbest_warmup_ratio = float(study.best_params['warmup_ratio'])\nbest_gradient_accumulation_steps = int(study.best_params['gradient_accumulation_steps'])\nbest_epoch = int(study.best_params['num_train_epochs'])\n\nprint_custom('Extract best study params')\nprint(f'The best learning rate is: {best_lr}')\nprint(f'The best weight decay is: {best_weight_decay}')\nprint(f'The best warmup ratio is: {best_warmup_ratio}')\nprint(f'The best gradient accumulation step is : {best_gradient_accumulation_steps}')\nprint(f'The best epoch is : {best_epoch}')\n\nprint_custom('Create dictionary of the best hyperparameters')\nbest_hp_dict = {\n    'best_learning_rate': best_lr,\n    'best_weight_decay': best_weight_decay,\n    'best_warmup_ratio': best_warmup_ratio,\n    'best_gradient_accumulation_steps': best_gradient_accumulation_steps,\n    'best_epoch': best_epoch\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T07:51:11.542364Z","iopub.execute_input":"2025-01-15T07:51:11.542706Z","iopub.status.idle":"2025-01-15T08:00:34.850972Z","shell.execute_reply.started":"2025-01-15T07:51:11.542671Z","shell.execute_reply":"2025-01-15T08:00:34.849879Z"}},"outputs":[{"name":"stderr","text":"[I 2025-01-15 07:51:11,550] A new study created in memory with name: hp-search\n","output_type":"stream"},{"name":"stdout","text":"\n\nTriggering Optuna study\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/879 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fda2ce355d74445bfe5dfb4ed34b951"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e824f5d3ec77463d9ae05af5437fee95"}},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [120/120 02:38, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"[I 2025-01-15 07:54:00,646] Trial 0 finished with value: 5.4509521484375 and parameters: {'learning_rate': 0.008738763408612403, 'weight_decay': 5.960386473276296e-05, 'warmup_ratio': 0.04452377040926211, 'gradient_accumulation_steps': 1, 'num_train_epochs': 3}. Best is trial 0 with value: 5.4509521484375.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [120/120 02:48, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"[I 2025-01-15 07:56:52,271] Trial 1 finished with value: 5.418101501464844 and parameters: {'learning_rate': 0.0033315096647181463, 'weight_decay': 4.1193418718520336e-05, 'warmup_ratio': 0.027063898887017348, 'gradient_accumulation_steps': 1, 'num_train_epochs': 3}. Best is trial 1 with value: 5.418101501464844.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [160/160 03:39, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"[I 2025-01-15 08:00:34,843] Trial 2 finished with value: 1.3733306884765626 and parameters: {'learning_rate': 3.979049360136424e-05, 'weight_decay': 0.0026403445952656534, 'warmup_ratio': 0.010395335112022534, 'gradient_accumulation_steps': 1, 'num_train_epochs': 4}. Best is trial 2 with value: 1.3733306884765626.\n","output_type":"stream"},{"name":"stdout","text":"\n\nFinding study best parameters\n----------------------------------------------------------------------------------------------------\n\n\nExtract best study params\n----------------------------------------------------------------------------------------------------\nThe best learning rate is: 3.979049360136424e-05\nThe best weight decay is: 0.0026403445952656534\nThe best warmup ratio is: 0.010395335112022534\nThe best gradient accumulation step is : 1\nThe best epoch is : 4\n\n\nCreate dictionary of the best hyperparameters\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":47},{"cell_type":"markdown","source":"## 3.5 Model training: the best hyperparameters","metadata":{}},{"cell_type":"code","source":"model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:05:26.213732Z","iopub.execute_input":"2025-01-15T08:05:26.214613Z","iopub.status.idle":"2025-01-15T08:05:26.366538Z","shell.execute_reply.started":"2025-01-15T08:05:26.214579Z","shell.execute_reply":"2025-01-15T08:05:26.365657Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"training_args_bp = TrainingArguments(\"mdeberta-squad-best-params\",\n                                     evaluation_strategy=\"no\",\n                                     save_strategy=\"epoch\",\n                                     logging_steps=300,\n                                     optim=\"adamw_torch\",\n                                     learning_rate=best_lr,\n                                     seed = RANDOM_SEED,\n                                     lr_scheduler_type='cosine',\n                                     fp16=True, #reduce the memory footprint - If you have an error as No space left on device during training&saving results\n                                     weight_decay=best_weight_decay,\n                                     warmup_ratio=best_warmup_ratio,\n                                     gradient_accumulation_steps=best_gradient_accumulation_steps,\n                                     num_train_epochs=best_epoch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:05:27.757554Z","iopub.execute_input":"2025-01-15T08:05:27.758273Z","iopub.status.idle":"2025-01-15T08:05:27.799734Z","shell.execute_reply.started":"2025-01-15T08:05:27.758238Z","shell.execute_reply":"2025-01-15T08:05:27.798954Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"trainer_bp = Trainer(\n    model,\n    training_args_bp,\n    train_dataset=train_ds,\n    eval_dataset=validation_ds,\n    tokenizer=tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:05:29.818273Z","iopub.execute_input":"2025-01-15T08:05:29.818868Z","iopub.status.idle":"2025-01-15T08:05:30.195131Z","shell.execute_reply.started":"2025-01-15T08:05:29.818836Z","shell.execute_reply":"2025-01-15T08:05:30.194440Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# os.environ[\"WANDB_DISABLED\"] = \"true\"\ntorch.cuda.empty_cache()\n\n# 15/01/2025\ntrainer_bp.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:05:38.932309Z","iopub.execute_input":"2025-01-15T08:05:38.933035Z","iopub.status.idle":"2025-01-15T08:47:13.320139Z","shell.execute_reply.started":"2025-01-15T08:05:38.933001Z","shell.execute_reply":"2025-01-15T08:47:13.319397Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2104' max='2104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2104/2104 41:32, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>300</td>\n      <td>1.789200</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.550500</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.303000</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.146800</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.981600</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.844500</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.809600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2104, training_loss=1.2028537315560837, metrics={'train_runtime': 2494.0298, 'train_samples_per_second': 13.495, 'train_steps_per_second': 0.844, 'total_flos': 6595771840229376.0, 'train_loss': 1.2028537315560837, 'epoch': 4.0})"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"# \"\"\"\n# Save the model\n# \"\"\"\n# trainer_bp.save_model('./mdeberta-finetuned-best_params-QA_15_01_25')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:52:05.626537Z","iopub.execute_input":"2025-01-15T08:52:05.626849Z","iopub.status.idle":"2025-01-15T08:52:05.630516Z","shell.execute_reply.started":"2025-01-15T08:52:05.626823Z","shell.execute_reply":"2025-01-15T08:52:05.629533Z"}},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":"## 3.6 Evaluation","metadata":{}},{"cell_type":"code","source":"predictions, _, _ = trainer_bp.predict(test_ds)\nstart_logits, end_logits = predictions\n\ncompute_metrics(start_logits, end_logits, test_ds, ds_final[\"test\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:50:59.569259Z","iopub.execute_input":"2025-01-15T08:50:59.569926Z","iopub.status.idle":"2025-01-15T08:52:00.246681Z","shell.execute_reply.started":"2025-01-15T08:50:59.569894Z","shell.execute_reply":"2025-01-15T08:52:00.245782Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d816e39220d548288b8e044eeedcb796"}},"metadata":{}},{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"{'exact_match': 61.35, 'f1': 80.77243935477296}"},"metadata":{}}],"execution_count":53},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"markdown","source":"# 4. Using the fine-tuned model (with base params)","metadata":{}},{"cell_type":"code","source":"# Replace this with your own checkpoint\nmodel_checkpoint = \"./bert-squad-base-params-QA_15_01_25\"\nquestion_answerer = pipeline(\"question-answering\", model=model_checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T09:54:27.064509Z","iopub.execute_input":"2025-01-15T09:54:27.065200Z","iopub.status.idle":"2025-01-15T09:54:27.667832Z","shell.execute_reply.started":"2025-01-15T09:54:27.065166Z","shell.execute_reply":"2025-01-15T09:54:27.666970Z"}},"outputs":[{"name":"stderr","text":"Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"context = \"\"\"\nВ начале XX века Тебриз был пристанищем множества радикальных организаций, в силу чего играл важную роль в Конституционной революции каджарского Ирана 1905—1911 годов.\nУже в 1908 году из города были изгнаны сторонники Мохаммед-Али шаха, который, будучи принцем, занимал пост губернатора Тебриза.\nОдним из главных требований протестующих было создание нового меджлиса.\nВ ответ на революцию, английские и русские войска вторглись в Иран, чтобы её подавить.\nПоследние заняли Тебриз, разоружили радикальные революционные группы, но при этом не признавали и отказывали возможности проехать в город шахскому губернатору.\nВ итоге Мохаммед Али-шах вынужден был уехать в Россию, а к власти в 1909 году пришёл новый правитель Султан Ахмад-шах, последний из династии Каджаров[4].\n\"\"\"\nquestion = \"Чьи сторонники были изгнаны в 1908 году во время протестов в Иране в начале XX века?\" # Мохаммед-Али шаха\nquestion_answerer(question=question, context=context)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T09:54:31.178916Z","iopub.execute_input":"2025-01-15T09:54:31.179526Z","iopub.status.idle":"2025-01-15T09:54:31.925325Z","shell.execute_reply.started":"2025-01-15T09:54:31.179493Z","shell.execute_reply":"2025-01-15T09:54:31.924298Z"}},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"{'score': 0.8873432874679565,\n 'start': 218,\n 'end': 237,\n 'answer': ' Мохаммед-Али шаха,'}"},"metadata":{}}],"execution_count":47},{"cell_type":"markdown","source":"Our model is working well!","metadata":{}},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"markdown","source":"**Some observations**:\n* As you can see, there is no any rapidly changes between metrics as the result of training model based on the base hyperparams VS hyperparams getting from automatic hyperparameter optimizations.\n\nIn order to improve the results it will be better to use more data for getting best hyperparams and training model if you have enough resources (time and GPU/CPU memory).","metadata":{}}]}